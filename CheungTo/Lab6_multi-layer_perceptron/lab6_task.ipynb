{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0PNevS86zMh"
      },
      "source": [
        "# **COMP 2211 Exploring Artificial Intelligence** #\n",
        "## Lab 6 - Multilayer Perceptron (MLP) ##\n",
        "![cnn_model.png](https://images.prismic.io/turing/659d78a0531ac2845a2742df_need_deep_neural_networks_5_11zon_54d2f93a48.webp?auto=format,compress)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRvz2pYK7Pa2"
      },
      "source": [
        "# **Lab Tasks**\n",
        "\n",
        "The objective of this lab is to create your custom MLP model to predict house prices in California.\n",
        "\n",
        "A brief rundown of the tasks is shown below:\n",
        "1. Importing and visualizing dataset\n",
        "2. Data preprocessing (Task 1)\n",
        "3. Model building (Task 2)\n",
        "4. Model compilation and training (Task 3)\n",
        "5. Model evaluation\n",
        "\n",
        "More information on each section will follow as you progress with your lab.\n",
        "\n",
        "Hope you enjoy the lab and best of luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxY2MkCn7pe9"
      },
      "source": [
        "# Importing and Visualizing the California Housing Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2NbzfE877tY"
      },
      "source": [
        "Let's take a look at the California Housing dataset, which will be used in this lab assignment to predict house prices using regression.\n",
        "\n",
        "This is a popular dataset which consists of 20,640 data samples, each described by a total of 9 features (8 features and 1 target label).\n",
        "\n",
        "Information on the 8 features are below:\n",
        "1. MedInc:        median income in block group\n",
        "2. HouseAge:      median house age in block group\n",
        "3. AveRooms:      average number of rooms per household\n",
        "4. AveBedrms:     average number of bedrooms per household\n",
        "5. Population:    block group population\n",
        "6. AveOccup:     average number of household members\n",
        "7. Latitude:     block group latitude\n",
        "8. Longitude:    block group longitude\n",
        "\n",
        "The target value is the Median House Value (MedHouseVal), expressed in hundreds of thousands of dollars.\n",
        "\n",
        "Further information on the dataset can be found here: https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html\n",
        "\n",
        "Run the code cells below to download and visualize your dataset.\n",
        "\n",
        "**Do not modify the code cells in this section.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_OpDo1T04f0"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras import optimizers\n",
        "from keras import regularizers\n",
        "import os\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "if __name__ == \"__main__\":\n",
        "  # Import libraries\n",
        "  import matplotlib.pyplot as plt\n",
        "  import seaborn as sns\n",
        "  import matplotlib.pyplot as plt\n",
        "  import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFrFYOap5pV1"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  # Fetch and download data from sklearn.\n",
        "  house_dataset = fetch_california_housing(data_home=\"./\", download_if_missing=True)\n",
        "\n",
        "  # The dataset is already seperated in X and y data\n",
        "  data = pd.DataFrame(house_dataset.data, columns=house_dataset.feature_names)\n",
        "  labels = pd.Series(house_dataset.target, name=\"MedHouseVal\")\n",
        "\n",
        "  # Let us combine the X and y data to have a single dataframe for easier visualization.\n",
        "  california_data = data.join(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xja492R-7LfG"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  california_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhaZihH48XM0",
        "outputId": "b85358c9-32ea-48a9-d741-ccfc832e9821"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  # Check attribute information\n",
        "  # As you can see, there are no null values and all are numeric.\n",
        "  california_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "id": "4gOoYvUC0U08",
        "outputId": "1e5d2d3e-38a3-4725-db07-1bb70fab5dcc"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  # Visualize distribution for each feature\n",
        "\n",
        "  california_data.hist(bins=50, figsize=(20,10))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 778
        },
        "id": "xpkne5DomeR-",
        "outputId": "c741d039-a136-4a12-f5ea-001f58561213"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  # Visualize correlation with respect to each attribute\n",
        "\n",
        "  plt.figure(figsize=(10, 8))\n",
        "  california_data['MedHouseVal'] = labels\n",
        "  sns.heatmap(california_data.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
        "  plt.title('Correlation Heatmap')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xu39zHRx-Frh"
      },
      "source": [
        "# Task 1 Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44cpOhCq-IUV"
      },
      "source": [
        "The following is what we have learnt from the data exploration above:\n",
        "1. No null values\n",
        "2. All are numeric data types (float64)\n",
        "3. A mix of linear and non-linear relationships is seen in the heatmap.\n",
        "\n",
        "According to the feature distributions and the heatmap, we cannot rely on traditional regression models such as Linear Regression due to the presence of non-linear relationships; we would need to use a model which can capture non-linear relationships -> MLP!\n",
        "\n",
        "Therefore, the major preprocessing we need to carry out is to normalize our data to standard normal distribution to avoid any bias towards certain features. Additionally, we need to split our data into train and test datasets before building the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9oVe_JWqLHl"
      },
      "outputs": [],
      "source": [
        "def standard_scalar(x, mean, std):\n",
        "  \"\"\"\n",
        "  Standardizes the input Pandas DataFrame using Z-score normalization.\n",
        "\n",
        "  This function normalizes the input data using the provided mean and standard deviation.\n",
        "  Standardization ensures that each feature has a mean of 0 and a standard deviation of 1,\n",
        "  which helps improve the performance of machine learning models, especially those relying\n",
        "  on gradient-based optimization.\n",
        "\n",
        "  Parameters:\n",
        "  ----------\n",
        "  data : pandas.DataFrame\n",
        "      The dataset containing feature values, where each column represents a feature,\n",
        "      and each row represents a sample.\n",
        "  mean : pandas.Series\n",
        "      The mean values of each feature, computed from the training set.\n",
        "      Using the training mean ensures consistency and prevents data leakage.\n",
        "  std : pandas.Series\n",
        "      The standard deviation of each feature, computed from the training set.\n",
        "      Avoids data imbalance and ensures a normalized distribution.\n",
        "\n",
        "  Returns:\n",
        "  ----------\n",
        "  standardized_data : pandas.DataFrame\n",
        "    The standardized dataset with each feature having a mean close to 0\n",
        "    and a standard deviation close to 1.\n",
        "  \"\"\"\n",
        "  ###############################################################################\n",
        "  # TODO: your code starts here\n",
        "\n",
        "  # TODO: your code ends here\n",
        "  ###############################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLyIT7VTpIsJ"
      },
      "outputs": [],
      "source": [
        "def preprocess(data):\n",
        "\n",
        "  ###############################################################################\n",
        "  # TODO: your code starts here\n",
        "  # Hint: When using train_test_split, please set random_state to 42 to ensure the same dataset split as zinc\n",
        "  # Hint: Please normalize both the training and test data using the mean and standard deviation of X and y from the training set. Normalizing y helps the model converge. We will grade based on this setting\n",
        "\n",
        "\n",
        "  # split data into features and target label\n",
        "\n",
        "  # split data into train and testing sets (use 20% for test size)\n",
        "\n",
        "  # normalize your dataset\n",
        "\n",
        "  # TODO: your code ends here\n",
        "  ###############################################################################\n",
        "\n",
        "  return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Srh3cjy3poM_",
        "outputId": "416dcbce-c655-4cdf-fc65-32acd392cfc0"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  # Run the function and print the corresponding shapes.\n",
        "  X_train, X_test, y_train, y_test = preprocess(california_data)\n",
        "  print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubgfwFOORpu7"
      },
      "source": [
        "# Task 2 Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdoYm2cCRzEB"
      },
      "source": [
        "It is now time to finally build our custom MLP model. You can use the following layers to create your model:\n",
        "\n",
        "* Fully-connected (`Dense`)\n",
        "* Dropout (`Dropout`)\n",
        "\n",
        "Additionally, feel free to play around with different activation functions, number of neurons, regularizers, number of hidden layers, etc.\n",
        "\n",
        "**Note that you do not have unlimited computing resources and you should avoid creating models that are too large to run on Google Colab.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gC3ZMH-XrIJp"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "  model = Sequential()\n",
        "\n",
        "  ###############################################################################\n",
        "  # TODO: your code starts here\n",
        "  # Hint: you need to take into account the input_shape for the first layer.\n",
        "  # Hint: Do not create models with more than 10,000 parameters. ZINC will not evaluate models with more than 10,000 parameters.\n",
        "\n",
        "  # TODO: your code ends here\n",
        "  ###############################################################################\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "ql4J3nN5zV-n",
        "outputId": "aa831916-2dab-46db-9200-5885480af9e0"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  # Your model summary\n",
        "  model = create_model()\n",
        "  model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwA6nLYeTn_3"
      },
      "source": [
        "# Task 3 Model Compilation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGHjHS8vWE2L"
      },
      "source": [
        "It is now time to complete the code below to compile your model before training.\n",
        "\n",
        "You can use any optimizer, loss function, and metrics to train your model. However, we recommend the following:\n",
        "1. Optimizer = Adam\n",
        "2. Loss = Mean Squared Error\n",
        "3. Metrics = Mean Absolute Error and Mean Squared Error (https://machinelearningmastery.com/regression-metrics-for-machine-learning/)\n",
        "\n",
        "Remember to also complete the model.fit function with your hyperparameters. While you are allowed to explore other options, we recommend having the number of epochs close to 100, if not more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEXTK5emZnq7"
      },
      "outputs": [],
      "source": [
        "def model_compile(model):\n",
        "  ###############################################################################\n",
        "  # TODO: your code starts here\n",
        "\n",
        "  # Fill the model.compile() function below\n",
        "\n",
        "  # TODO: your code ends here\n",
        "  ###############################################################################\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ct9BW6f3Zs9b"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  model = model_compile(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCnOpomjr5IW",
        "outputId": "b2fc8490-ed06-4570-cfe1-b505fbe8c2e1"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  # You can adjust the epochs, batch size, and validation_split to achieve better results.\n",
        "  history = model.fit(X_train, y_train, epochs=150, batch_size=64, validation_split=0.2)\n",
        "  model.save_weights(\"mlp_model.weights.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjlteV5eXo2R"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLoz0_Cfzo2s"
      },
      "source": [
        "It is now time to run the code cells below to generate predictions of house prices. This will then be evaluated with the testing labels using 3 metrics: Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and R2 score.\n",
        "\n",
        "The R2 score is a statistical measure that tells us how well our model fits the data. It has a range between 0 and 1, with 1 indicating that our model fits perfectly well with the data. It is important to note that a negative R2 value means your model has not understood the data distribution at all.\n",
        "\n",
        "Finally, you can run the last code cell to plot an Actual vs. Predicted values graph. This data visualization technique is very useful in regression tasks, as it showcases how well the predictions fit the regressed diagonal line. If your prediction points are close to the diagonal line, it means you have a high R2 score.\n",
        "\n",
        "**Do not change the code cells in this section.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyjK8AdYvuvJ",
        "outputId": "21a22280-f00e-4865-eb23-e27826cab1ac"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  # Predict median house prices based on testing data\n",
        "  # model.load_weights(\"mlp_model.weights.h5\")\n",
        "  y_pred = model.predict(X_test).flatten()\n",
        "\n",
        "  # Generate MSE, RMSE, and R2 scores.\n",
        "  mse = mean_squared_error(y_test, y_pred)\n",
        "  rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "  r2 = r2_score(y_test, y_pred)\n",
        "  # The r2 results should be the same locally and on Zinc, as long as the code is complete and follows the requirements.\n",
        "  print(\"mse =\", mse, \", rmse =\", rmse, \", r2 =\", r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "5aRbXnMzvwd-",
        "outputId": "53137c8e-95c7-4b62-9766-017aefe14490"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  # Plot Actual Vs. Predicted Median House Values\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "  plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')  # Diagonal line\n",
        "  plt.title('Actual vs Predicted Median House Value')\n",
        "  plt.xlabel('Actual Values')\n",
        "  plt.ylabel('Predicted Values')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmTuMuYd37mx"
      },
      "source": [
        "## **Grading Scheme**\n",
        "\n",
        "Please export your notebook on Colab as `lab6_tasks.py` (File -> Download -> Download .py), and submit it together with your `mlp_model.weights.h5` model weight file.\n",
        "\n",
        "\n",
        "* You get **3 points** for data preprocessing (task 1)\n",
        "* You get **2 points** for the valid implementation of the MLP model (task 2)\n",
        "* You get **1 point** for model compilation (task 3)\n",
        "* You get **1 point** for achieving an R2 score of at least 0.70\n",
        "* You get **2 points** for achieving an R2 score of at least 0.75\n",
        "* You get **3 points** (full mark) for achieving an R2 score of at least 0.78"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
